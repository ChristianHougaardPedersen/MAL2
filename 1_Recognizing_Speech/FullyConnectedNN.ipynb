{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.load('x_digits.npy')\n",
    "Y = np.load('y_digits.npy')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape([-1, 129, 71, 1])\n",
    "X_test = X_test.reshape([-1, 129, 71, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have the data separated, we will split the train for the validation inside the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with very simple fully connected NN (Neural Network). We will have just three hidden layers, with activation ReLu and Adam optimizer. For all of the NNs we will use 'sparse_categorical_crossentropy' loss function as this is a classification problem. The NN will output 10 different probablities of the given input being a specific number. More about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.2723 - loss: 5.5723 - val_accuracy: 0.4243 - val_loss: 2.2670\n",
      "Epoch 2/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.4500 - loss: 1.9334 - val_accuracy: 0.5590 - val_loss: 1.5598\n",
      "Epoch 3/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.5459 - loss: 1.5884 - val_accuracy: 0.6034 - val_loss: 1.3059\n",
      "Epoch 4/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5876 - loss: 1.3821 - val_accuracy: 0.6388 - val_loss: 1.1997\n",
      "Epoch 5/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6557 - loss: 1.0954 - val_accuracy: 0.6101 - val_loss: 1.2124\n",
      "Epoch 6/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6820 - loss: 0.9983 - val_accuracy: 0.6651 - val_loss: 1.0661\n",
      "Epoch 7/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6928 - loss: 0.9944 - val_accuracy: 0.6581 - val_loss: 1.1176\n",
      "Epoch 8/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7217 - loss: 0.8804 - val_accuracy: 0.6715 - val_loss: 1.0039\n",
      "Epoch 9/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7338 - loss: 0.7961 - val_accuracy: 0.6971 - val_loss: 0.9516\n",
      "Epoch 10/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7518 - loss: 0.7653 - val_accuracy: 0.7265 - val_loss: 0.8598\n",
      "Epoch 11/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7580 - loss: 0.7110 - val_accuracy: 0.7243 - val_loss: 0.8612\n",
      "Epoch 12/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7532 - loss: 0.7443 - val_accuracy: 0.6915 - val_loss: 1.0144\n",
      "Epoch 13/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7752 - loss: 0.6743 - val_accuracy: 0.7437 - val_loss: 0.7934\n",
      "Epoch 14/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7830 - loss: 0.6490 - val_accuracy: 0.7385 - val_loss: 0.8152\n",
      "Epoch 15/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7910 - loss: 0.6251 - val_accuracy: 0.7439 - val_loss: 0.8012\n",
      "Epoch 16/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8018 - loss: 0.5957 - val_accuracy: 0.7564 - val_loss: 0.7451\n",
      "Epoch 17/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8097 - loss: 0.5606 - val_accuracy: 0.7543 - val_loss: 0.7983\n",
      "Epoch 18/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8114 - loss: 0.5539 - val_accuracy: 0.7557 - val_loss: 0.7876\n",
      "Epoch 19/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8223 - loss: 0.5270 - val_accuracy: 0.7648 - val_loss: 0.7784\n",
      "Epoch 20/20\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8227 - loss: 0.5262 - val_accuracy: 0.7481 - val_loss: 0.8269\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74% for the validation, nice. However, we can see that the difference in accuracy and loss between train and validation is significant which may suggest overfitting. Let's use AdamW as optimizer to apply some built-in regularization.\n",
    "           \n",
    "(Comment from ChatGPT about the usage of AdamW:      \n",
    "Better generalization: The direct weight decay helps prevent weights from growing too large during training. By penalizing large weights, the model is encouraged to find simpler solutions, improving generalization to the validation/test sets and reducing overfitting.\n",
    "\n",
    "Reduces overfitting impact: Proper weight decay leads to better regularization, helping the model avoid fitting to noise or irrelevant patterns in the training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.2802 - loss: 7.4380 - val_accuracy: 0.4408 - val_loss: 2.0133\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.4399 - loss: 2.0635 - val_accuracy: 0.4731 - val_loss: 1.8141\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5234 - loss: 1.5710 - val_accuracy: 0.5734 - val_loss: 1.4581\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5783 - loss: 1.4064 - val_accuracy: 0.5983 - val_loss: 1.3549\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6087 - loss: 1.3209 - val_accuracy: 0.6315 - val_loss: 1.1808\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6397 - loss: 1.1375 - val_accuracy: 0.6262 - val_loss: 1.1963\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6565 - loss: 1.0439 - val_accuracy: 0.5518 - val_loss: 1.7419\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6784 - loss: 0.9756 - val_accuracy: 0.6476 - val_loss: 1.1062\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6908 - loss: 0.9140 - val_accuracy: 0.6399 - val_loss: 1.1475\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7003 - loss: 0.8714 - val_accuracy: 0.6725 - val_loss: 1.0369\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7182 - loss: 0.8279 - val_accuracy: 0.6864 - val_loss: 0.9475\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7302 - loss: 0.7825 - val_accuracy: 0.6876 - val_loss: 0.9610\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7384 - loss: 0.7602 - val_accuracy: 0.7081 - val_loss: 0.8829\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7537 - loss: 0.7181 - val_accuracy: 0.7343 - val_loss: 0.8299\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7481 - loss: 0.7363 - val_accuracy: 0.7176 - val_loss: 0.8787\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7613 - loss: 0.6962 - val_accuracy: 0.7215 - val_loss: 0.8783\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7684 - loss: 0.6656 - val_accuracy: 0.6878 - val_loss: 1.0425\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.7741 - loss: 0.6681 - val_accuracy: 0.7136 - val_loss: 0.9185\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7704 - loss: 0.6655 - val_accuracy: 0.7009 - val_loss: 0.9764\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7830 - loss: 0.6196 - val_accuracy: 0.7053 - val_loss: 0.9909\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7889 - loss: 0.6022 - val_accuracy: 0.7295 - val_loss: 0.8700\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8001 - loss: 0.5872 - val_accuracy: 0.7399 - val_loss: 0.8499\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8038 - loss: 0.5685 - val_accuracy: 0.7211 - val_loss: 0.9243\n",
      "Epoch 24/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7946 - loss: 0.5822 - val_accuracy: 0.7186 - val_loss: 0.9608\n",
      "Epoch 25/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8085 - loss: 0.5422 - val_accuracy: 0.7488 - val_loss: 0.8899\n",
      "Epoch 26/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8108 - loss: 0.5494 - val_accuracy: 0.7258 - val_loss: 0.9308\n",
      "Epoch 27/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8200 - loss: 0.5234 - val_accuracy: 0.7409 - val_loss: 0.9071\n",
      "Epoch 28/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8253 - loss: 0.5125 - val_accuracy: 0.7371 - val_loss: 0.9153\n",
      "Epoch 29/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8288 - loss: 0.4978 - val_accuracy: 0.7502 - val_loss: 0.8705\n",
      "Epoch 30/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8347 - loss: 0.4720 - val_accuracy: 0.7693 - val_loss: 0.7910\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.005)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it did sligthly better but it is still overfitting, let's try increasing the weight decay two times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.2476 - loss: 5.8731 - val_accuracy: 0.3750 - val_loss: 2.3753\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.3607 - loss: 2.3506 - val_accuracy: 0.3973 - val_loss: 1.8316\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.4198 - loss: 1.8850 - val_accuracy: 0.4526 - val_loss: 1.7538\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.4711 - loss: 1.6564 - val_accuracy: 0.4831 - val_loss: 1.6498\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4909 - loss: 1.5914 - val_accuracy: 0.5155 - val_loss: 1.5205\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.5246 - loss: 1.4292 - val_accuracy: 0.5318 - val_loss: 1.3765\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.5493 - loss: 1.3099 - val_accuracy: 0.5231 - val_loss: 1.4170\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.5560 - loss: 1.2709 - val_accuracy: 0.5513 - val_loss: 1.2773\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5732 - loss: 1.2187 - val_accuracy: 0.5594 - val_loss: 1.2728\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.5901 - loss: 1.1753 - val_accuracy: 0.5722 - val_loss: 1.2431\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5981 - loss: 1.1444 - val_accuracy: 0.5743 - val_loss: 1.2426\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6069 - loss: 1.1263 - val_accuracy: 0.5762 - val_loss: 1.2613\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6215 - loss: 1.0928 - val_accuracy: 0.5825 - val_loss: 1.2306\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6217 - loss: 1.1296 - val_accuracy: 0.6122 - val_loss: 1.1521\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6404 - loss: 1.0414 - val_accuracy: 0.6155 - val_loss: 1.1329\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6499 - loss: 1.0198 - val_accuracy: 0.6264 - val_loss: 1.1264\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6571 - loss: 0.9962 - val_accuracy: 0.6201 - val_loss: 1.1250\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6649 - loss: 0.9802 - val_accuracy: 0.6350 - val_loss: 1.1015\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6701 - loss: 0.9475 - val_accuracy: 0.6209 - val_loss: 1.1398\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6750 - loss: 0.9398 - val_accuracy: 0.6444 - val_loss: 1.0829\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6708 - loss: 0.9789 - val_accuracy: 0.6518 - val_loss: 1.0363\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6978 - loss: 0.8733 - val_accuracy: 0.6394 - val_loss: 1.0680\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6923 - loss: 0.8983 - val_accuracy: 0.6425 - val_loss: 1.1149\n",
      "Epoch 24/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7047 - loss: 0.8604 - val_accuracy: 0.6497 - val_loss: 1.1010\n",
      "Epoch 25/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7136 - loss: 0.8381 - val_accuracy: 0.6406 - val_loss: 1.1136\n",
      "Epoch 26/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7167 - loss: 0.8253 - val_accuracy: 0.6567 - val_loss: 1.0619\n",
      "Epoch 27/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7198 - loss: 0.8086 - val_accuracy: 0.6660 - val_loss: 1.0222\n",
      "Epoch 28/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7248 - loss: 0.8031 - val_accuracy: 0.6643 - val_loss: 1.0308\n",
      "Epoch 29/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7331 - loss: 0.7819 - val_accuracy: 0.6632 - val_loss: 1.0410\n",
      "Epoch 30/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7401 - loss: 0.7571 - val_accuracy: 0.6764 - val_loss: 1.0275\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.01)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this has slightly corrected the relation between loss and val-loss, but the relation between accuracy and val-accuracy is still the same. The only difference is that now we might underfitting as the overall acurracy has dipped 10%. Let's try meeting halfway and set the weight decay to 0.0075."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.2411 - loss: 5.9462 - val_accuracy: 0.4296 - val_loss: 1.9344\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.4228 - loss: 1.9505 - val_accuracy: 0.4461 - val_loss: 1.7504\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.4497 - loss: 1.8743 - val_accuracy: 0.4450 - val_loss: 1.8845\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.4838 - loss: 1.6717 - val_accuracy: 0.4504 - val_loss: 1.7301\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5024 - loss: 1.5194 - val_accuracy: 0.5075 - val_loss: 1.5381\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5300 - loss: 1.3941 - val_accuracy: 0.5483 - val_loss: 1.3225\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5575 - loss: 1.2707 - val_accuracy: 0.5513 - val_loss: 1.3132\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.5857 - loss: 1.1926 - val_accuracy: 0.5643 - val_loss: 1.2776\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6074 - loss: 1.1271 - val_accuracy: 0.5895 - val_loss: 1.1995\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6241 - loss: 1.0766 - val_accuracy: 0.5511 - val_loss: 1.3269\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6344 - loss: 1.0478 - val_accuracy: 0.6097 - val_loss: 1.1710\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6472 - loss: 1.0049 - val_accuracy: 0.6081 - val_loss: 1.1421\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6603 - loss: 0.9780 - val_accuracy: 0.6029 - val_loss: 1.1862\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6656 - loss: 0.9513 - val_accuracy: 0.6408 - val_loss: 1.1065\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6706 - loss: 0.9548 - val_accuracy: 0.6448 - val_loss: 1.0450\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6887 - loss: 0.9016 - val_accuracy: 0.6501 - val_loss: 1.0213\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6922 - loss: 0.8865 - val_accuracy: 0.6366 - val_loss: 1.1469\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7009 - loss: 0.8637 - val_accuracy: 0.6471 - val_loss: 1.0621\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7044 - loss: 0.8376 - val_accuracy: 0.6539 - val_loss: 1.0358\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7055 - loss: 0.8305 - val_accuracy: 0.6809 - val_loss: 0.9777\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7112 - loss: 0.8537 - val_accuracy: 0.6704 - val_loss: 1.0081\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7158 - loss: 0.8132 - val_accuracy: 0.6806 - val_loss: 0.9955\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7241 - loss: 0.7965 - val_accuracy: 0.6874 - val_loss: 0.9634\n",
      "Epoch 24/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.7342 - loss: 0.7631 - val_accuracy: 0.6613 - val_loss: 1.0913\n",
      "Epoch 25/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7393 - loss: 0.7516 - val_accuracy: 0.6818 - val_loss: 0.9986\n",
      "Epoch 26/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7485 - loss: 0.7343 - val_accuracy: 0.6865 - val_loss: 0.9579\n",
      "Epoch 27/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7557 - loss: 0.7055 - val_accuracy: 0.6829 - val_loss: 0.9942\n",
      "Epoch 28/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7602 - loss: 0.6869 - val_accuracy: 0.6944 - val_loss: 0.9683\n",
      "Epoch 29/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7639 - loss: 0.6827 - val_accuracy: 0.6881 - val_loss: 0.9892\n",
      "Epoch 30/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7592 - loss: 0.6894 - val_accuracy: 0.6823 - val_loss: 1.0159\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to see which weight decay value performs the best:   \n",
    "\n",
    "* 0: accuracy: 0.8227 - loss: 0.5262 - val_accuracy: 0.7481 - val_loss: 0.8269\n",
    "* 0.005: accuracy: 0.8347 - loss: 0.4720 - val_accuracy: 0.7693 - val_loss: 0.7910\n",
    "* 0.0075: accuracy: 0.7592 - loss: 0.6894 - val_accuracy: 0.6823 - val_loss: 1.0159\n",
    "* 0.01: accuracy: 0.7401 - loss: 0.7571 - val_accuracy: 0.6764 - val_loss: 1.0275\n",
    "\n",
    "We can clearly see that 0.005 performs the best, gives the best accuracy and lowest loss values. Let's go with this one then. Let's add some callbacks to see if we can still reduce some overfitting. We are going to add early stopping and Learning rate scheduler. Let's remove one layer too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.2557 - loss: 6.1764 - val_accuracy: 0.4599 - val_loss: 1.6256 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.4882 - loss: 1.5545 - val_accuracy: 0.5581 - val_loss: 1.3270 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.6032 - loss: 1.1675 - val_accuracy: 0.6457 - val_loss: 1.0175 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.6658 - loss: 0.9630 - val_accuracy: 0.6820 - val_loss: 0.9267 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7091 - loss: 0.8782 - val_accuracy: 0.7101 - val_loss: 0.8627 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7462 - loss: 0.7508 - val_accuracy: 0.7560 - val_loss: 0.7732 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7638 - loss: 0.7014 - val_accuracy: 0.7264 - val_loss: 0.8648 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7824 - loss: 0.6458 - val_accuracy: 0.7641 - val_loss: 0.7557 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7991 - loss: 0.5842 - val_accuracy: 0.7448 - val_loss: 0.8127 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7987 - loss: 0.5885 - val_accuracy: 0.7755 - val_loss: 0.7249 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8248 - loss: 0.5192 - val_accuracy: 0.7711 - val_loss: 0.7327 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.8217 - loss: 0.5180 - val_accuracy: 0.8088 - val_loss: 0.6140 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.8446 - loss: 0.4569 - val_accuracy: 0.7988 - val_loss: 0.6317 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.8475 - loss: 0.4524 - val_accuracy: 0.8028 - val_loss: 0.6466 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8856 - loss: 0.3328 - val_accuracy: 0.8195 - val_loss: 0.6657 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9004 - loss: 0.2962 - val_accuracy: 0.8355 - val_loss: 0.6327 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(125, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(75, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(40, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Adding callbacks improved the situation as well as adding some extra layers with number of neurons formed in a funnel. We have actually changed the weight decay to 0.0075 as it performs better with more layers now. Let's see if we can reduce the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.2786 - loss: 5.3940 - val_accuracy: 0.4106 - val_loss: 1.8056 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5060 - loss: 1.4617 - val_accuracy: 0.5887 - val_loss: 1.2743 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6222 - loss: 1.1256 - val_accuracy: 0.6022 - val_loss: 1.1517 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6765 - loss: 0.9525 - val_accuracy: 0.6573 - val_loss: 1.0550 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6949 - loss: 0.9280 - val_accuracy: 0.7108 - val_loss: 0.8545 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.7276 - loss: 0.8144 - val_accuracy: 0.7523 - val_loss: 0.7595 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.7556 - loss: 0.7360 - val_accuracy: 0.7600 - val_loss: 0.7458 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.7628 - loss: 0.7012 - val_accuracy: 0.7243 - val_loss: 0.8695 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.7737 - loss: 0.6756 - val_accuracy: 0.7474 - val_loss: 0.7942 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.8296 - loss: 0.5187 - val_accuracy: 0.8035 - val_loss: 0.6040 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.8423 - loss: 0.4653 - val_accuracy: 0.7920 - val_loss: 0.6744 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.8524 - loss: 0.4391 - val_accuracy: 0.7969 - val_loss: 0.6573 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8806 - loss: 0.3572 - val_accuracy: 0.8339 - val_loss: 0.5290 - learning_rate: 2.5000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8925 - loss: 0.3196 - val_accuracy: 0.8320 - val_loss: 0.5404 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.8973 - loss: 0.2968 - val_accuracy: 0.8337 - val_loss: 0.5477 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - accuracy: 0.9112 - loss: 0.2581 - val_accuracy: 0.8411 - val_loss: 0.5223 - learning_rate: 1.2500e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2369 - val_accuracy: 0.8425 - val_loss: 0.5247 - learning_rate: 1.2500e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9256 - loss: 0.2189 - val_accuracy: 0.8428 - val_loss: 0.5231 - learning_rate: 1.2500e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9322 - loss: 0.2019 - val_accuracy: 0.8484 - val_loss: 0.5106 - learning_rate: 6.2500e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9374 - loss: 0.1869 - val_accuracy: 0.8472 - val_loss: 0.5178 - learning_rate: 6.2500e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9403 - loss: 0.1799 - val_accuracy: 0.8476 - val_loss: 0.5204 - learning_rate: 6.2500e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9424 - loss: 0.1716 - val_accuracy: 0.8516 - val_loss: 0.5171 - learning_rate: 3.1250e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9445 - loss: 0.1659 - val_accuracy: 0.8523 - val_loss: 0.5186 - learning_rate: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(150, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(25, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this didn't work out well. Let's go other way, add two more layers and increase the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.2368 - loss: 6.5989 - val_accuracy: 0.5385 - val_loss: 1.4101 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - accuracy: 0.5508 - loss: 1.3667 - val_accuracy: 0.6718 - val_loss: 1.0056 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.6528 - loss: 1.0452 - val_accuracy: 0.7095 - val_loss: 0.8799 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.7201 - loss: 0.8362 - val_accuracy: 0.7279 - val_loss: 0.8394 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.7496 - loss: 0.7459 - val_accuracy: 0.7557 - val_loss: 0.7209 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.7811 - loss: 0.6530 - val_accuracy: 0.7772 - val_loss: 0.7302 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.7972 - loss: 0.6135 - val_accuracy: 0.7644 - val_loss: 0.6987 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - accuracy: 0.8149 - loss: 0.5489 - val_accuracy: 0.7948 - val_loss: 0.6308 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.8313 - loss: 0.4940 - val_accuracy: 0.7985 - val_loss: 0.6329 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8457 - loss: 0.4692 - val_accuracy: 0.7753 - val_loss: 0.7151 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.8782 - loss: 0.3504 - val_accuracy: 0.8530 - val_loss: 0.4637 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9002 - loss: 0.2912 - val_accuracy: 0.8534 - val_loss: 0.4723 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9123 - loss: 0.2578 - val_accuracy: 0.8449 - val_loss: 0.5098 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9323 - loss: 0.1926 - val_accuracy: 0.8725 - val_loss: 0.4576 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9433 - loss: 0.1573 - val_accuracy: 0.8770 - val_loss: 0.4705 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9503 - loss: 0.1385 - val_accuracy: 0.8788 - val_loss: 0.4656 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.9632 - loss: 0.1061 - val_accuracy: 0.8898 - val_loss: 0.4254 - learning_rate: 1.2500e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9718 - loss: 0.0803 - val_accuracy: 0.8862 - val_loss: 0.4687 - learning_rate: 1.2500e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.9741 - loss: 0.0745 - val_accuracy: 0.8912 - val_loss: 0.4835 - learning_rate: 1.2500e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step - accuracy: 0.9805 - loss: 0.0570 - val_accuracy: 0.8974 - val_loss: 0.4810 - learning_rate: 6.2500e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9857 - loss: 0.0452 - val_accuracy: 0.8942 - val_loss: 0.5046 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(400, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(300, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(150, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's definitely overfitting. Let's reduce the layers and number of neurons to be slighty above the two steps before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.2486 - loss: 4.8609 - val_accuracy: 0.4829 - val_loss: 1.5324 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.5277 - loss: 1.4197 - val_accuracy: 0.6173 - val_loss: 1.1616 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.6406 - loss: 1.0830 - val_accuracy: 0.7051 - val_loss: 0.8908 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7044 - loss: 0.8897 - val_accuracy: 0.7399 - val_loss: 0.8219 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.7423 - loss: 0.7698 - val_accuracy: 0.7637 - val_loss: 0.7699 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.7768 - loss: 0.6588 - val_accuracy: 0.7753 - val_loss: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.7924 - loss: 0.6087 - val_accuracy: 0.7979 - val_loss: 0.6164 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8159 - loss: 0.5515 - val_accuracy: 0.8051 - val_loss: 0.6153 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8236 - loss: 0.5347 - val_accuracy: 0.7760 - val_loss: 0.7371 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8332 - loss: 0.5075 - val_accuracy: 0.8295 - val_loss: 0.5462 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8528 - loss: 0.4412 - val_accuracy: 0.8025 - val_loss: 0.6225 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8569 - loss: 0.4311 - val_accuracy: 0.8125 - val_loss: 0.6556 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.9002 - loss: 0.3037 - val_accuracy: 0.8502 - val_loss: 0.4789 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9125 - loss: 0.2553 - val_accuracy: 0.8611 - val_loss: 0.4855 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9132 - loss: 0.2556 - val_accuracy: 0.8474 - val_loss: 0.5511 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.9366 - loss: 0.1869 - val_accuracy: 0.8711 - val_loss: 0.4564 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9496 - loss: 0.1510 - val_accuracy: 0.8730 - val_loss: 0.4760 - learning_rate: 2.5000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9547 - loss: 0.1339 - val_accuracy: 0.8734 - val_loss: 0.4865 - learning_rate: 2.5000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9609 - loss: 0.1152 - val_accuracy: 0.8834 - val_loss: 0.4558 - learning_rate: 1.2500e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9676 - loss: 0.0959 - val_accuracy: 0.8858 - val_loss: 0.4628 - learning_rate: 1.2500e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9731 - loss: 0.0824 - val_accuracy: 0.8807 - val_loss: 0.4906 - learning_rate: 1.2500e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.0750 - val_accuracy: 0.8860 - val_loss: 0.4803 - learning_rate: 6.2500e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9801 - loss: 0.0642 - val_accuracy: 0.8876 - val_loss: 0.4962 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(250, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(150, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not good, let's try early stopping on val_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.2793 - loss: 6.5954 - val_accuracy: 0.5103 - val_loss: 1.6185 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.5402 - loss: 1.5098 - val_accuracy: 0.6494 - val_loss: 1.0709 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.6444 - loss: 1.0674 - val_accuracy: 0.6801 - val_loss: 0.9679 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7074 - loss: 0.8786 - val_accuracy: 0.6946 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7354 - loss: 0.7921 - val_accuracy: 0.7395 - val_loss: 0.7604 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7632 - loss: 0.7025 - val_accuracy: 0.7483 - val_loss: 0.7442 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.7852 - loss: 0.6350 - val_accuracy: 0.7495 - val_loss: 0.8053 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8011 - loss: 0.5887 - val_accuracy: 0.7771 - val_loss: 0.7006 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.8120 - loss: 0.5499 - val_accuracy: 0.7867 - val_loss: 0.6814 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8245 - loss: 0.5152 - val_accuracy: 0.7611 - val_loss: 0.7418 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8358 - loss: 0.4830 - val_accuracy: 0.7997 - val_loss: 0.6157 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8417 - loss: 0.4582 - val_accuracy: 0.7944 - val_loss: 0.6422 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8560 - loss: 0.4227 - val_accuracy: 0.8142 - val_loss: 0.5800 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8635 - loss: 0.4064 - val_accuracy: 0.8048 - val_loss: 0.6146 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8733 - loss: 0.3759 - val_accuracy: 0.8118 - val_loss: 0.6324 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9047 - loss: 0.2844 - val_accuracy: 0.8539 - val_loss: 0.4940 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9237 - loss: 0.2179 - val_accuracy: 0.8528 - val_loss: 0.4904 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9279 - loss: 0.2048 - val_accuracy: 0.8486 - val_loss: 0.5395 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9334 - loss: 0.1951 - val_accuracy: 0.8458 - val_loss: 0.5542 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9509 - loss: 0.1494 - val_accuracy: 0.8772 - val_loss: 0.4572 - learning_rate: 2.5000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.9639 - loss: 0.1081 - val_accuracy: 0.8807 - val_loss: 0.4833 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9667 - loss: 0.0995 - val_accuracy: 0.8828 - val_loss: 0.4875 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9723 - loss: 0.0796 - val_accuracy: 0.8900 - val_loss: 0.4797 - learning_rate: 1.2500e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9781 - loss: 0.0656 - val_accuracy: 0.8891 - val_loss: 0.4989 - learning_rate: 1.2500e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9815 - loss: 0.0565 - val_accuracy: 0.8925 - val_loss: 0.4997 - learning_rate: 6.2500e-05\n",
      "Epoch 26/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9843 - loss: 0.0491 - val_accuracy: 0.8939 - val_loss: 0.5119 - learning_rate: 6.2500e-05\n",
      "Epoch 27/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9865 - loss: 0.0437 - val_accuracy: 0.8956 - val_loss: 0.5182 - learning_rate: 3.1250e-05\n",
      "Epoch 28/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9877 - loss: 0.0405 - val_accuracy: 0.8948 - val_loss: 0.5274 - learning_rate: 3.1250e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9886 - loss: 0.0379 - val_accuracy: 0.8953 - val_loss: 0.5332 - learning_rate: 1.5625e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9896 - loss: 0.0361 - val_accuracy: 0.8937 - val_loss: 0.5408 - learning_rate: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(250, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(150, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also doesn't work, let's add BatchNormalization layer to the NN with 4 hidden layers.\n",
    "\n",
    "(ChatGPT: Batch Normalization: Add batch normalization layers to stabilize and accelerate training, reduce overfitting, and help with generalization. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.3785 - loss: 1.7941 - val_accuracy: 0.5839 - val_loss: 1.1941 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.6607 - loss: 0.9796 - val_accuracy: 0.7200 - val_loss: 0.8295 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7357 - loss: 0.7563 - val_accuracy: 0.7181 - val_loss: 0.8450 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7841 - loss: 0.6305 - val_accuracy: 0.7437 - val_loss: 0.7733 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8107 - loss: 0.5510 - val_accuracy: 0.7557 - val_loss: 0.7417 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8406 - loss: 0.4629 - val_accuracy: 0.7725 - val_loss: 0.6888 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8630 - loss: 0.4054 - val_accuracy: 0.7799 - val_loss: 0.6834 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8720 - loss: 0.3717 - val_accuracy: 0.7655 - val_loss: 0.7740 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8856 - loss: 0.3334 - val_accuracy: 0.7751 - val_loss: 0.7499 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9194 - loss: 0.2378 - val_accuracy: 0.8499 - val_loss: 0.4884 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9419 - loss: 0.1765 - val_accuracy: 0.8506 - val_loss: 0.4911 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9529 - loss: 0.1427 - val_accuracy: 0.8434 - val_loss: 0.5638 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9625 - loss: 0.1174 - val_accuracy: 0.8748 - val_loss: 0.4485 - learning_rate: 2.5000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9772 - loss: 0.0786 - val_accuracy: 0.8679 - val_loss: 0.4850 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9814 - loss: 0.0653 - val_accuracy: 0.8677 - val_loss: 0.5035 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9845 - loss: 0.0531 - val_accuracy: 0.8705 - val_loss: 0.4815 - learning_rate: 1.2500e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9901 - loss: 0.0387 - val_accuracy: 0.8730 - val_loss: 0.4874 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(125, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(75, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(40, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.0075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 12% difference, overifitting, not satisfied. The batchNormalization did not work well. Let's try droput layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.1010 - loss: 4.5751 - val_accuracy: 0.1256 - val_loss: 2.3335 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.1213 - loss: 2.2875 - val_accuracy: 0.1303 - val_loss: 2.2580 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.1254 - loss: 2.2586 - val_accuracy: 0.1584 - val_loss: 2.2084 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1460 - loss: 2.2249 - val_accuracy: 0.1870 - val_loss: 2.1275 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1834 - loss: 2.1224 - val_accuracy: 0.2205 - val_loss: 2.0117 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.2176 - loss: 2.0329 - val_accuracy: 0.2463 - val_loss: 1.9536 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.2390 - loss: 1.9671 - val_accuracy: 0.2712 - val_loss: 1.8690 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.2698 - loss: 1.9050 - val_accuracy: 0.3420 - val_loss: 1.7445 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.3598 - loss: 1.6773 - val_accuracy: 0.4866 - val_loss: 1.4103 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.4511 - loss: 1.4666 - val_accuracy: 0.5168 - val_loss: 1.3126 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.4958 - loss: 1.3746 - val_accuracy: 0.5662 - val_loss: 1.1970 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.5479 - loss: 1.2331 - val_accuracy: 0.6241 - val_loss: 1.0145 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6164 - loss: 1.0769 - val_accuracy: 0.7018 - val_loss: 0.8884 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6491 - loss: 0.9924 - val_accuracy: 0.6953 - val_loss: 0.8695 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.6724 - loss: 0.9284 - val_accuracy: 0.7465 - val_loss: 0.7687 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7012 - loss: 0.8727 - val_accuracy: 0.7506 - val_loss: 0.7407 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7250 - loss: 0.7916 - val_accuracy: 0.7842 - val_loss: 0.6516 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7458 - loss: 0.7342 - val_accuracy: 0.7904 - val_loss: 0.6459 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7594 - loss: 0.7121 - val_accuracy: 0.7850 - val_loss: 0.6581 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7588 - loss: 0.6984 - val_accuracy: 0.8111 - val_loss: 0.5761 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7864 - loss: 0.6287 - val_accuracy: 0.8104 - val_loss: 0.5615 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7825 - loss: 0.6392 - val_accuracy: 0.8200 - val_loss: 0.5579 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7898 - loss: 0.6037 - val_accuracy: 0.8088 - val_loss: 0.5753 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7966 - loss: 0.6007 - val_accuracy: 0.8197 - val_loss: 0.5508 - learning_rate: 0.0010\n",
      "Epoch 25/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8003 - loss: 0.5907 - val_accuracy: 0.8183 - val_loss: 0.5583 - learning_rate: 0.0010\n",
      "Epoch 26/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8111 - loss: 0.5597 - val_accuracy: 0.8025 - val_loss: 0.5878 - learning_rate: 0.0010\n",
      "Epoch 27/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8350 - loss: 0.4773 - val_accuracy: 0.8532 - val_loss: 0.4467 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8541 - loss: 0.4247 - val_accuracy: 0.8541 - val_loss: 0.4474 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8601 - loss: 0.4148 - val_accuracy: 0.8479 - val_loss: 0.4672 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m713/713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.8750 - loss: 0.3609 - val_accuracy: 0.8693 - val_loss: 0.4097 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape = [129, 71]))\n",
    "model.add(tf.keras.layers.Flatten()) #Flattens a multidim input to 1D so it can be used for Dense layers\n",
    "model.add(tf.keras.layers.Dense(200, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(125, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(75, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(40, activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.001, weight_decay=0.075)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "early_stopping_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler= tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience= 2)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 30, validation_split = 0.2, callbacks=[early_stopping_cb, lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEAAAAH!! Randomly droping neurons in the shallow layers was the key for finding model which doesn't overfit. The model runs longer now, but it performs better. This is our FINAL model for fully connected NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9311 - loss: 0.2181\n",
      "Loss + accuracy on train data: [0.2505785822868347, 0.9210286140441895]\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8593 - loss: 0.4124\n",
      "Loss + accuracy on test data: [0.425045371055603, 0.8575838208198547]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss + accuracy on train data: {}\".format(model.evaluate(X_train, Y_train)))\n",
    "print(\"Loss + accuracy on test data: {}\".format(model.evaluate(X_test, Y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
