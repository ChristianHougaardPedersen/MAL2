{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('x_letters.npy')\n",
    "Y = np.load('y_letters.npy')\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_dim=28):\n",
    "    # Input layers\n",
    "    image_input = layers.Input(shape=(img_dim, img_dim))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "\n",
    "    # Flattening the image so that it can go into Dense 1D layers\n",
    "    flat_image = layers.Flatten()(image_input)\n",
    "\n",
    "    # Concatenate flattened image with label input\n",
    "    concat = layers.Concatenate()([flat_image, label_input])\n",
    "\n",
    "    # Feedforward network\n",
    "    x = layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")(concat)\n",
    "    x = layers.Dense(15, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    # Output layer\n",
    "    real_or_fake = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Building the model\n",
    "    return tf.keras.Model([image_input, label_input], real_or_fake)\n",
    "\n",
    "\n",
    "def build_generator(img_dim=28, latent_dim=32):\n",
    "    latent_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(1,))\n",
    "    \n",
    "    concat = layers.Concatenate()([latent_input, label_input])\n",
    "\n",
    "    x = layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\")(concat)\n",
    "    x = layers.Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(250, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(28*28, activation=\"sigmoid\")(x)\n",
    "\n",
    "    generated_image = layers.Reshape((img_dim, img_dim))(x)\n",
    "    \n",
    "    return tf.keras.Model([latent_input, label_input], generated_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://keras.io/examples/generative/dcgan_overriding_train_step/\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, real_labels = data\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "\n",
    "        # Creating random points and labels\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim),\n",
    "            seed = self.seed_generator\n",
    "        )\n",
    "        random_labels = tf.random.uniform(\n",
    "            shape=(batch_size,), minval=0, maxval=26, dtype=tf.int64\n",
    "        )\n",
    "\n",
    "        #Combine real and fake - images and labels\n",
    "        generated_images = self.generator([random_latent_vectors, random_labels])\n",
    "\n",
    "        combined_images = ops.concatenate([real_images, generated_images], axis=0)\n",
    "        combined_labels = ops.concatenate([real_labels, random_labels], axis=0)\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        real_or_fake_labels = ops.concatenate(\n",
    "            [ops.ones((batch_size, 1))\n",
    "            , ops.zeros((batch_size, 1))], \n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Add random noise to the labels - important trick!\n",
    "        #basically makes sure the discriminator is not perfect,\n",
    "        #so that the generator never has a chance to learn\n",
    "        real_or_fake_labels += 0.2 * tf.random.uniform(tf.shape(real_or_fake_labels))\n",
    "\n",
    "        # ---Train the discriminator---\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator([combined_images, combined_labels])\n",
    "            d_loss = self.loss_fn(real_or_fake_labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Creating random points and labels\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim),\n",
    "            seed = self.seed_generator\n",
    "        )\n",
    "        random_labels = tf.random.uniform(\n",
    "            shape=(batch_size,), minval=0, maxval=26, dtype=tf.int64\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = ops.ones((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator([random_latent_vectors, random_labels])\n",
    "            predictions = self.discriminator([generated_images, random_labels])\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = build_discriminator()\n",
    "gen = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - d_loss: -0.4075 - g_loss: 4.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -4.5682 - g_loss: 5.8647\n",
      "Epoch 3/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 0.9902 - g_loss: 2.1346\n",
      "Epoch 4/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -5.0004 - g_loss: 3.6064\n",
      "Epoch 5/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 5.3890 - g_loss: 1.3653\n",
      "Epoch 6/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 8.9724 - g_loss: 2.6215\n",
      "Epoch 7/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 1.0805 - g_loss: 2.3806\n",
      "Epoch 8/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 4.9107 - g_loss: 1.2206\n",
      "Epoch 9/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 12.0582 - g_loss: 0.7375\n",
      "Epoch 10/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 13.5087 - g_loss: 1.1265\n",
      "Epoch 11/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -5.7039 - g_loss: 4.3101\n",
      "Epoch 12/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 13.1190 - g_loss: 0.8751\n",
      "Epoch 13/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 3.4582 - g_loss: 1.0514\n",
      "Epoch 14/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 1.9127 - g_loss: 1.5271\n",
      "Epoch 15/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 2.5096 - g_loss: 0.9223\n",
      "Epoch 16/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 0.1401 - g_loss: 2.4535\n",
      "Epoch 17/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 2.7099 - g_loss: 0.6910\n",
      "Epoch 18/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 1.0456 - g_loss: 1.7166\n",
      "Epoch 19/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -0.4256 - g_loss: 2.8159\n",
      "Epoch 20/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 1.5554 - g_loss: 1.3761\n",
      "Epoch 21/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -2.3195 - g_loss: 2.3310\n",
      "Epoch 22/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -4.6298 - g_loss: 2.8556\n",
      "Epoch 23/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 1.0548 - g_loss: 2.5395\n",
      "Epoch 24/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: 12.5241 - g_loss: 3.3719\n",
      "Epoch 25/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -7.3557 - g_loss: 4.8947\n",
      "Epoch 26/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -4.1282 - g_loss: 10.4871\n",
      "Epoch 27/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -38.4529 - g_loss: 6.4351\n",
      "Epoch 28/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -73.2344 - g_loss: 14.8965\n",
      "Epoch 29/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -149.8170 - g_loss: 20.7725\n",
      "Epoch 30/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -221.6547 - g_loss: 55.5307\n",
      "Epoch 31/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - d_loss: -331.8081 - g_loss: 6.7769\n",
      "Epoch 32/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 250.3289 - g_loss: 0.4860\n",
      "Epoch 33/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 440.0082 - g_loss: 5.1078\n",
      "Epoch 34/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 21.9811 - g_loss: 8.2414\n",
      "Epoch 35/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -122.8154 - g_loss: 4.3185\n",
      "Epoch 36/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -177.6232 - g_loss: 4.1183\n",
      "Epoch 37/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -161.0766 - g_loss: 5.7120\n",
      "Epoch 38/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 106.2296 - g_loss: 5.8623\n",
      "Epoch 39/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 49.5581 - g_loss: 1.7862\n",
      "Epoch 40/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 92.9652 - g_loss: 1.9430\n",
      "Epoch 41/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 0.0011 - g_loss: 2.9006\n",
      "Epoch 42/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -42.1968 - g_loss: 4.1398\n",
      "Epoch 43/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 148.3683 - g_loss: 5.1205\n",
      "Epoch 44/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 183.2723 - g_loss: 1.9702\n",
      "Epoch 45/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 415.6702 - g_loss: 1.3800\n",
      "Epoch 46/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - d_loss: 65.9884 - g_loss: 1.0368\n",
      "Epoch 47/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -13.3964 - g_loss: 4.2204\n",
      "Epoch 48/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - d_loss: 20.6304 - g_loss: 8.4044\n",
      "Epoch 49/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: 3.3114 - g_loss: 3.2246\n",
      "Epoch 50/50\n",
      "\u001b[1m2064/2064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - d_loss: -1.4957 - g_loss: 3.6557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19f5ee48510>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "gan = GAN(discriminator=dis, generator=gen, latent_dim=32)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.RMSprop(learning_rate=0.00009, clipvalue=1.0),\n",
    "    g_optimizer=keras.optimizers.RMSprop(learning_rate=0.002),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(X_train,y_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x450 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(n_images, generator, latent_dim):\n",
    "    random_latent_vectors = keras.random.normal(\n",
    "            shape=(n_images, latent_dim), seed = keras.random.SeedGenerator(21)\n",
    "        )\n",
    "    random_labels = tf.random.uniform(\n",
    "            shape=(n_images,), minval=0, maxval=26, dtype=tf.int64\n",
    "        )\n",
    "    \n",
    "    images = generator([random_latent_vectors, random_labels])\n",
    "\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 4.5))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(1, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_images(12, gan.generator, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
